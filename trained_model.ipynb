{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69b3cc2a-8831-4008-9891-085138b95c60",
   "metadata": {},
   "source": [
    "## Project: Rental Apartment Analysis Tel Aviv - Jaffa- Machine Learning â€“ Part 3\n",
    "### Presented by:  \n",
    "### Amichay Nager -316225986 \n",
    "### Tair Mimon-322240615\n",
    "### github: https://github.com/amichay1234/project_part2-/blob/main/Machine%20Learning%20%E2%80%93%20Part%202%20Final.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c42198d-2c91-450e-b7ce-e7972e0c365e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "from sklearn.model_selection import cross_val_score, KFold,LeaveOneOut, LeavePOut, GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import mean_absolute_error ,mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, make_scorer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import pickle\n",
    "import requests\n",
    "import time\n",
    "import getpass\n",
    "import googlemaps\n",
    "import movecolumn as mc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d3a4201-ae1e-42d5-9176-f9bca41bb2e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>property_type</th>\n",
       "      <th>neighborhood</th>\n",
       "      <th>address</th>\n",
       "      <th>room_num</th>\n",
       "      <th>floor</th>\n",
       "      <th>area</th>\n",
       "      <th>garden_area</th>\n",
       "      <th>days_to_enter</th>\n",
       "      <th>num_of_payments</th>\n",
       "      <th>monthly_arnona</th>\n",
       "      <th>...</th>\n",
       "      <th>ac</th>\n",
       "      <th>handicap</th>\n",
       "      <th>has_bars</th>\n",
       "      <th>has_safe_room</th>\n",
       "      <th>has_balcony</th>\n",
       "      <th>is_furnished</th>\n",
       "      <th>is_renovated</th>\n",
       "      <th>num_of_images</th>\n",
       "      <th>distance_from_center</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>×“×™×¨×”</td>\n",
       "      <td>×”×¦×¤×•×Ÿ ×”×™×©×Ÿ ×”×—×œ×§ ×”××¨×›×–×™</td>\n",
       "      <td>××”×¨\"×œ 25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>71</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>467.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1005.0</td>\n",
       "      <td>10150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>×“×™×¨×”</td>\n",
       "      <td>×”×¦×¤×•×Ÿ ×”×™×©×Ÿ ×”×—×œ×§ ×”××¨×›×–×™</td>\n",
       "      <td>××¨×œ×•×–×•×¨×•×‘ 35</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>70</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>253.0</td>\n",
       "      <td>6600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>×“×™×¨×”</td>\n",
       "      <td>×”×¦×¤×•×Ÿ ×”×™×©×Ÿ ×”×—×œ×§ ×”××¨×›×–×™</td>\n",
       "      <td>×•×•×¨××™×–×” 5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1</td>\n",
       "      <td>65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>740.0</td>\n",
       "      <td>9000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>×“×™×¨×”</td>\n",
       "      <td>×”×¦×¤×•×Ÿ ×”×™×©×Ÿ ×”×—×œ×§ ×”××¨×›×–×™</td>\n",
       "      <td>×¢×× ×•××œ ×”×¨×•××™ 30</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1206.0</td>\n",
       "      <td>5800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>×“×™×¨×”</td>\n",
       "      <td>×”×¦×¤×•×Ÿ ×”×™×©×Ÿ ×”×—×œ×§ ×”××¨×›×–×™</td>\n",
       "      <td>××¨×œ×•×–×•×¨×•×‘ 50</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>70</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>7700.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  property_type            neighborhood          address  room_num floor  \\\n",
       "0          ×“×™×¨×”  ×”×¦×¤×•×Ÿ ×”×™×©×Ÿ ×”×—×œ×§ ×”××¨×›×–×™         ××”×¨\"×œ 25       3.0     2   \n",
       "1          ×“×™×¨×”  ×”×¦×¤×•×Ÿ ×”×™×©×Ÿ ×”×—×œ×§ ×”××¨×›×–×™     ××¨×œ×•×–×•×¨×•×‘ 35       3.0     1   \n",
       "2          ×“×™×¨×”  ×”×¦×¤×•×Ÿ ×”×™×©×Ÿ ×”×—×œ×§ ×”××¨×›×–×™        ×•×•×¨××™×–×” 5       2.5     1   \n",
       "3          ×“×™×¨×”  ×”×¦×¤×•×Ÿ ×”×™×©×Ÿ ×”×—×œ×§ ×”××¨×›×–×™  ×¢×× ×•××œ ×”×¨×•××™ 30       2.0     3   \n",
       "4          ×“×™×¨×”  ×”×¦×¤×•×Ÿ ×”×™×©×Ÿ ×”×—×œ×§ ×”××¨×›×–×™     ××¨×œ×•×–×•×¨×•×‘ 50       3.0     1   \n",
       "\n",
       "   area  garden_area  days_to_enter  num_of_payments  monthly_arnona  ...  ac  \\\n",
       "0    71          NaN            0.0             12.0           467.0  ...   1   \n",
       "1    70          NaN            0.0             12.0           240.0  ...   1   \n",
       "2    65          NaN            NaN             12.0           400.0  ...   1   \n",
       "3    40          NaN            0.0             12.0           100.0  ...   0   \n",
       "4    70          NaN            0.0             11.0           250.0  ...   1   \n",
       "\n",
       "   handicap has_bars  has_safe_room  has_balcony  is_furnished  is_renovated  \\\n",
       "0         0        0              1            1             0             0   \n",
       "1         0        1              0            1             0             0   \n",
       "2         1        0              0            1             0             1   \n",
       "3         0        0              0            0             0             0   \n",
       "4         0        1              0            0             0             1   \n",
       "\n",
       "   num_of_images  distance_from_center    price  \n",
       "0            6.0                1005.0  10150.0  \n",
       "1            3.0                 253.0   6600.0  \n",
       "2            8.0                 740.0   9000.0  \n",
       "3            2.0                1206.0   5800.0  \n",
       "4            5.0                 255.0   7700.0  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the file\n",
    "file_name = r'https://raw.githubusercontent.com/amichay1234/project_part2-/refs/heads/main/train.csv'\n",
    "dataset = pd.read_csv(file_name)\n",
    "\n",
    "# New column order\n",
    "new_order = [\n",
    "    \"property_type\", \"neighborhood\", \"address\", \"room_num\", \"floor\", \"area\", \"garden_area\", \"days_to_enter\", \"num_of_payments\",\n",
    "    \"monthly_arnona\", \"building_tax\", \"total_floors\", \"description\", \"has_parking\", \"has_storage\", \"elevator\", \"ac\", \"handicap\", \"has_bars\",\n",
    "    \"has_safe_room\", \"has_balcony\", \"is_furnished\", \"is_renovated\", \"num_of_images\", \"distance_from_center\", \"price\"]\n",
    "\n",
    "# Change the order of the columns accordingly.\n",
    "dataset = dataset[new_order]\n",
    "\n",
    "# Displaying the first 5 lines\n",
    "dataset.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a1c2351-b578-4354-af11-07f0b0ae6b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 26\n",
      "Number of columns: 788\n"
     ]
    }
   ],
   "source": [
    "num_rows = dataset.shape[1]\n",
    "print(f\"Number of rows: {num_rows}\")\n",
    "\n",
    "num_columns = dataset.shape[0]\n",
    "print(f\"Number of columns: {num_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c57dd341-7118-4456-9261-2a7d54fc5b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data_test=dataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d75cd47-71ac-4ac9-91b0-73f1e35e3f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(dataset):\n",
    "\n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    if 'price' in dataset.columns:\n",
    "        before = len(dataset)\n",
    "        dataset.dropna(subset=['price'], inplace=True)\n",
    "        print(f\"âŒ ×”×•×¡×¨×• {before - len(dataset)} ×©×•×¨×•×ª ×œ×œ× price\")\n",
    "\n",
    "        # ×©×œ×‘ 2 â€“ ×©××™×¨×” ×¢×œ ×˜×•×•×— ××—×™×¨×™× ×¡×‘×™×¨\n",
    "        before = len(dataset)\n",
    "        dataset = dataset[(dataset['price'] >= 1500) & (dataset['price'] <= 18500)]\n",
    "        print(f\"âŒ ×”×•×¡×¨×• {before - len(dataset)} ×©×•×¨×•×ª ×¢× ××—×™×¨ ××—×•×¥ ×œ×˜×•×•×— 1500â€“18500\")\n",
    "    else:\n",
    "        print(\"âš ï¸ ×¢××•×“×ª price ×œ× ×§×™×™××ª â€“ ×“×™×œ×•×’ ×¢×œ ×©×œ×‘×™ ×¡×™× ×•×Ÿ ×œ×¤×™ ××—×™×¨\")\n",
    "\n",
    "    # ×©×œ×‘ 3 â€“ ×”××¨×” ×©×œ ×ª×•×•×™× ×—×¡×¨×™ ××™×“×¢ ×œÖ¾NaN\n",
    "    dataset.replace({'': np.nan, ' ': np.nan}, inplace=True)\n",
    "    \n",
    "    # ×©×œ×‘ 4 â€“ ×”×¡×¨×ª ×©×•×¨×•×ª ×‘×œ×™ neighborhood ×•Ö¾address ×’× ×™×—×“\n",
    "    before = len(dataset)\n",
    "    dataset = dataset[~(dataset[\"neighborhood\"].isna() & dataset[\"address\"].isna())].reset_index(drop=True)\n",
    "    print(f\"âŒ ×”×•×¡×¨×• {before - len(dataset)} ×©×•×¨×•×ª ×©×‘×”×Ÿ ×’× neighborhood ×•×’× address ×—×¡×¨×™×\")\n",
    "\n",
    "    # ×©×œ×‘ 5 â€“ ×”×¡×¨×ª ×©×•×¨×•×ª ×¢× ×¤×—×•×ª ×Ö¾5 ×¢×¨×›×™× (×©××™× × 0 ××• NaN)\n",
    "    before = len(dataset)\n",
    "    dataset = dataset[dataset.apply(lambda row: (row != 0).sum() > 4, axis=1)]\n",
    "    print(f\"âŒ ×”×•×¡×¨×• {before - len(dataset)} ×©×•×¨×•×ª ×¢× 4 ×¢×¨×›×™× ××• ×¤×—×•×ª (×œ× 0 ×•×œ× NaN)\")\n",
    "    # ×¡×™×›×•× ×›×•×œ×œ\n",
    "   # print(f\"\\nğŸ“Š ×¡×š ×”×›×•×œ × ×©××¨×• {len(dataset)} ×©×•×¨×•×ª ××ª×•×š {start_rows} ×”××§×•×¨×™×•×ª.\")\n",
    "\n",
    "#################################################################################################################################\n",
    "\n",
    "    # ğŸ”¹ ×©××™×¨×” ×¢×œ ×›××•×ª ×©×•×¨×•×ª ×œ×¤× ×™ ×¢×™×‘×•×“\n",
    "    start_rows = len(dataset)\n",
    "    \n",
    "    # ğŸ”¹ ×©×œ×‘ 1: ×¡×™×•×•×’ ×œ×¤×™ property_type\n",
    "    def classify_property_type(val): \n",
    "        if not isinstance(val, str):\n",
    "            return np.nan\n",
    "        val_lower = val.lower()\n",
    "        if '×¡××‘×œ×˜' in val_lower:\n",
    "            return '×¡××‘×œ×˜'\n",
    "        elif '×¤×¨×˜×™' in val_lower or '×§×•×˜×’' in val_lower or '×“×• ××©×¤×—×ª×™' in val_lower:\n",
    "            return '×¤×¨×˜×™'\n",
    "        elif '×¡×˜×•×“×™×•' in val_lower or '×œ×•×¤×˜' in val_lower:\n",
    "            return '×¡×˜×•×“×™×•/×œ×•×¤×˜'\n",
    "        elif '×’×’' in val_lower or '×¤× ×˜×”××•×–' in val_lower:\n",
    "            return '×’×’/×¤× ×˜×”××•×–'\n",
    "        elif '×“×•×¤×œ×§×¡' in val_lower:\n",
    "            return '×“×•×¤×œ×§×¡'\n",
    "        elif '×“×™×¨×ª ×’×Ÿ' in val_lower:\n",
    "            return '×“×™×¨×ª ×’×Ÿ'\n",
    "        elif '×“×™×¨×”' in val_lower or '×“×™×¨×ª' in val_lower:\n",
    "            return '×“×™×¨×”'\n",
    "        else:\n",
    "            return np.nan\n",
    "    \n",
    "    # ğŸ”¹ ×©×œ×‘ 2: × ×™×—×•×© ×œ×¤×™ description\n",
    "    def guess_type_from_description(desc):\n",
    "        if pd.isna(desc):\n",
    "            return np.nan\n",
    "        desc = desc.lower()\n",
    "        non_apartment_keywords = [\n",
    "            r'××©×¨×“', r'××©×¨×“×™×', r'×§×œ×™× ×™×§×”', r'×’×œ×¨×™×”', r'×¡×˜×•×“×™×• ×œ×××Ÿ',\n",
    "            r'×—×œ×œ ×¢×‘×•×“×”', r'×ª×œ×ª ×¤××–×™', r'××—×¡×Ÿ', r'×¡×“× ×”', r'×‘×™×ª ××œ××›×”', r'××¡×—×¨×™'\n",
    "        ]\n",
    "        for pattern in non_apartment_keywords:\n",
    "            if re.search(rf'\\b{pattern}\\b', desc):\n",
    "                return np.nan\n",
    "        patterns = [\n",
    "            (r'\\b×™×—×™×“×ª ×“×™×•×¨\\b', '×™×—×™×“×ª ×“×™×•×¨'),\n",
    "            (r'\\b×¡××‘×œ×˜\\b', '×¡××‘×œ×˜'),\n",
    "            (r'\\b(×¤×¨×˜×™|×§×•×˜×’|×“×• ××©×¤×—×ª×™)\\b', '×¤×¨×˜×™'),\n",
    "            (r'\\b(×¡×˜×•×“×™×•|×œ×•×¤×˜)\\b', '×¡×˜×•×“×™×•/×œ×•×¤×˜'),\n",
    "            (r'\\b(×’×’|×¤× ×˜×”××•×–)\\b', '×’×’/×¤× ×˜×”××•×–'),\n",
    "            (r'\\b×“×•×¤×œ×§×¡\\b', '×“×•×¤×œ×§×¡'),\n",
    "            (r'\\b×“×™×¨×ª ×’×Ÿ\\b', '×“×™×¨×ª ×’×Ÿ'),\n",
    "            (r'\\b(×“×™×¨×”|×“×™×¨×ª)\\b', '×“×™×¨×”'),\n",
    "            (r'\\b×—×“×¨\\b', '×“×™×¨×”'),\n",
    "            (r'\\b×‘×™×ª\\b', '×“×™×¨×”'),\n",
    "        ]\n",
    "        for pattern, value in patterns:\n",
    "            if re.search(pattern, desc):\n",
    "                return value\n",
    "        return np.nan\n",
    "    \n",
    "    # ğŸ”¹ ×”×—×œ×” ×¢×œ ×”×“××˜×”\n",
    "    dataset['property_type_clean'] = dataset['property_type'].apply(classify_property_type)\n",
    "    \n",
    "    # ğŸ”¹ ×©×œ×‘ ×”×©×œ××” ×œ×¤×™ ×ª×™××•×¨\n",
    "    missing_mask = dataset['property_type_clean'].isna()\n",
    "    dataset.loc[missing_mask, 'property_type_clean'] = dataset.loc[missing_mask, 'description'].apply(guess_type_from_description)\n",
    "    \n",
    "    # ğŸ”¹ ××™×œ×•×™ \"×œ× ×™×“×•×¢\"\n",
    "    dataset['property_type_clean'] = dataset['property_type_clean'].fillna('×œ× ×™×“×•×¢')\n",
    "    \n",
    "    # ğŸ”¹ ×¡×™× ×•×Ÿ ×©×•×¨×•×ª ×¢× \"×§×•×¨×•×ª ×—×™×™×\"\n",
    "    before = len(dataset)\n",
    "    dataset = dataset[~dataset['description'].str.contains('×§×•×¨×•×ª ×—×™×™×', case=False, na=False)]\n",
    "    print(f\"âŒ ×”×•×¡×¨×• {before - len(dataset)} ×©×•×¨×•×ª ×¢× ×”×ª×™×™×—×¡×•×ª ×œ'×§×•×¨×•×ª ×—×™×™×'\")\n",
    "    \n",
    "    # ğŸ”¹ ×”×¡×¨×ª ×©×•×¨×•×ª ×¢× '×œ× ×™×“×•×¢'\n",
    "    before = len(dataset)\n",
    "    dataset = dataset[dataset['property_type_clean'] != '×œ× ×™×“×•×¢']\n",
    "    print(f\"âŒ ×”×•×¡×¨×• {before - len(dataset)} ×©×•×¨×•×ª ×©×‘×”×Ÿ ×¡×•×’ ×”× ×›×¡ ×œ× ×–×•×”×” (×œ× ×™×“×•×¢)\")\n",
    "    \n",
    "    # ğŸ”¹ ×¡×™×›×•×\n",
    "    print(f\"\\nğŸ“Š ×¡×š ×”×›×•×œ × ×©××¨×• {len(dataset)} ×©×•×¨×•×ª ××ª×•×š {start_rows} ×”××§×•×¨×™×•×ª.\")\n",
    "\n",
    "####################################################################################################################################\n",
    "    def split_address(address):\n",
    "        if not isinstance(address, str):\n",
    "            return pd.Series([None, None])\n",
    "        match = re.search(r'(\\D+)\\s*(\\d+)?', address)\n",
    "        if match:\n",
    "            street = match.group(1).strip()\n",
    "            number = match.group(2) if match.group(2) else None\n",
    "            return pd.Series([street, number])\n",
    "        return pd.Series([None, None])\n",
    "\n",
    "    dataset[['street', 'house_number']] = dataset['address'].apply(split_address)\n",
    "    dataset.drop('house_number', axis=1, inplace=True)\n",
    "\n",
    "    conditions = (dataset['floor'].isna()) & (dataset['property_type'].isin(['×¤×¨×˜×™']))\n",
    "    dataset.loc[conditions, 'floor'] = 0\n",
    "    dataset['street'] = dataset['street'].fillna(\"Unknown\")\n",
    "######################################################################################################################\n",
    "    def extract_floor_clean(val):\n",
    "        if pd.isna(val):\n",
    "            return np.nan\n",
    "        val = str(val).strip()\n",
    "        if val == '×§×¨×§×¢ ××ª×•×š ×§×¨×§×¢':\n",
    "            return 0\n",
    "        if '×§×¨×§×¢' in val:\n",
    "            return 0\n",
    "        if '××¨×ª×£' in val:\n",
    "            return -1\n",
    "        match = re.findall(r'\\d+', val)\n",
    "        if match:\n",
    "            try:\n",
    "                num = int(match[0])\n",
    "                if 0 <= num <= 100:\n",
    "                    return num\n",
    "            except:\n",
    "                return np.nan\n",
    "        return np.nan\n",
    "\n",
    "    dataset['floor_clean'] = dataset.apply(lambda row: extract_floor_clean(row['floor']), axis=1)\n",
    "    dataset['floor_clean'] = pd.to_numeric(dataset['floor_clean'], errors='coerce')\n",
    "    condition = (dataset['total_floors'] == 0) & (dataset['floor_clean'].isin([0, 1]))\n",
    "    dataset.loc[condition, 'total_floors'] = 1\n",
    "\n",
    "    def extract_floor_strict(val, total_floors):\n",
    "        if pd.isnull(val):\n",
    "            return np.nan\n",
    "        val = str(val).strip()\n",
    "        if val == '0' or '×§×¨×§×¢' in val:\n",
    "            return 1\n",
    "        match = re.search(r'\\d+', val)\n",
    "        if not match:\n",
    "            return np.nan\n",
    "        num_str = match.group()\n",
    "        try:\n",
    "            num = int(num_str)\n",
    "        except:\n",
    "            return np.nan\n",
    "        if 0 < num <= 100:\n",
    "            return num\n",
    "        if not pd.isnull(total_floors):\n",
    "            tf_str = str(int(total_floors))\n",
    "            if num_str.endswith(tf_str):\n",
    "                floor_part = num_str[:-len(tf_str)]\n",
    "                if floor_part.isdigit():\n",
    "                    floor_val = int(floor_part)\n",
    "                    if 0 < floor_val <= 100:\n",
    "                        return floor_val\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "    dataset['floor_clean'] = dataset.apply(\n",
    "        lambda row: extract_floor_strict(row['floor'], row['total_floors']),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "# ×¤×•× ×§×¦×™×” ×œ×—×™×œ×•×¥ ××¡×¤×¨ ×§×•××” ××”×ª×™××•×¨\n",
    "    def extract_floor_from_description(desc):\n",
    "        if pd.isna(desc):\n",
    "            return np.nan\n",
    "        desc = desc.lower()\n",
    "\n",
    "        # ×§×•××” ××¡×¤×¨×™×ª ×¨×’×™×œ×”: \"×§×•××” 3\", \"×‘×§×•××” 10\", \"×§×•××”: 5\"\n",
    "        match = re.search(r'×§×•×[×”|×ª] (?:××¡\\' )?(\\d{1,2})', desc)\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "\n",
    "        # ×§×•××” ××¢×œ ××©×”×• â€“ ×œ×“×•×’××” \"××¢×œ ×§×•××” 20\"\n",
    "        match = re.search(r'××¢×œ.*?(\\d{1,2})', desc)\n",
    "        if match:\n",
    "            return int(match.group(1)) + 1\n",
    "\n",
    "        # ×§×•××ª ×§×¨×§×¢\n",
    "        if '×§×•××ª ×§×¨×§×¢' in desc or '×§×¨×§×¢' in desc:\n",
    "            return 0\n",
    "\n",
    "        return np.nan\n",
    "\n",
    "    # ×©××™×¨×ª ×¢×•×ª×§ ×©×œ ×”×¢××•×“×” ×œ×¤× ×™ ×”×©×™× ×•×™×™×\n",
    "    original_floor = dataset['floor_clean'].copy()\n",
    "\n",
    "    # ×¡×™× ×•×Ÿ ×©×•×¨×•×ª ×¢× ×¢×¨×š ×—×¡×¨ ×‘×§×•××”\n",
    "    missing_floor_mask = dataset['floor_clean'].isna()\n",
    "    dataset.loc[missing_floor_mask, 'floor_clean'] = dataset.loc[missing_floor_mask, 'description'].apply(extract_floor_from_description)\n",
    "\n",
    "    \n",
    "    dataset.dropna(subset=['floor_clean'], inplace=True) #1\n",
    "\n",
    "\n",
    "    def fix_merged_floor(row):\n",
    "        floor = row['floor_clean']\n",
    "        total = row['total_floors']\n",
    "        if pd.notna(floor) and pd.notna(total):\n",
    "            if floor > total:\n",
    "                digits = [int(d) for d in str(int(floor)) if d.isdigit()]\n",
    "                if int(total) in digits:\n",
    "                    digits.remove(int(total))\n",
    "                    if digits:\n",
    "                        new_floor = digits[0]\n",
    "                        if new_floor <= total:\n",
    "                            return new_floor\n",
    "        return floor\n",
    "\n",
    "    dataset['floor_clean'] = dataset.apply(fix_merged_floor, axis=1)\n",
    "\n",
    "###############################################################################################################\n",
    "    mask = (\n",
    "        (dataset['floor_clean'] > dataset['total_floors']) &\n",
    "        (dataset['floor_clean'].notna()) &\n",
    "        (dataset['total_floors'].notna())\n",
    "    )\n",
    "    dataset.loc[mask, 'total_floors'] = np.nan\n",
    "\n",
    "    def extract_total_floors_from_floor(val):\n",
    "        if pd.isna(val):\n",
    "            return np.nan\n",
    "        val = str(val).strip()\n",
    "        if val == '×§×¨×§×¢ ××ª×•×š ×§×¨×§×¢':\n",
    "            return 1\n",
    "        if '×§×¨×§×¢ ××ª×•×š' in val:\n",
    "            match = re.search(r'×§×¨×§×¢ ××ª×•×š (\\d+)', val)\n",
    "            if match:\n",
    "                return int(match.group(1))\n",
    "            else:\n",
    "                return np.nan\n",
    "        match = re.findall(r'\\d+', val)\n",
    "        if len(match) >= 2:\n",
    "            return int(match[-1])\n",
    "        return np.nan\n",
    "\n",
    "    mask_missing = dataset['total_floors'].isna()\n",
    "    dataset.loc[mask_missing, 'total_floors'] = dataset.loc[mask_missing, 'floor'].apply(extract_total_floors_from_floor)\n",
    "    dataset['total_floors'] = dataset['total_floors'].astype(float)\n",
    "\n",
    "    # âœ… ×©×œ×‘ 1: ×™×¦×™×¨×ª ×§×˜×’×•×¨×™×•×ª ×–×× ×™×•×ª ×©×œ ××™×¡×™ ×‘× ×™×™×Ÿ\n",
    "    dataset['_building_tax_bin'] = pd.qcut(dataset['building_tax'], q=4, duplicates='drop')\n",
    "    \n",
    "    # âœ… ×©×œ×‘ 2: ×—×™×©×•×‘ ×—×¦×™×•×Ÿ total_floors ×œ×¤×™ elevator + has_parking + tax_bin\n",
    "    group_median = dataset.groupby(['elevator', 'has_parking', '_building_tax_bin'])['total_floors'].median()\n",
    "    \n",
    "    # âœ… ×©×œ×‘ 3: ×¤×•× ×§×¦×™×™×ª ××™×œ×•×™ ×—×›××” ×©××ª×—×©×‘×ª ×’× ×‘×§×•××” ×‘×¤×•×¢×œ\n",
    "    def smart_fill(row):\n",
    "        if pd.notna(row['total_floors']):\n",
    "            return row['total_floors']\n",
    "    \n",
    "        try:\n",
    "            group_value = group_median.loc[row['elevator'], row['has_parking'], row['_building_tax_bin']]\n",
    "        except:\n",
    "            group_value = dataset['total_floors'].median()\n",
    "    \n",
    "        if pd.notna(row.get('floor_clean')):\n",
    "            return max(group_value, row['floor_clean'])\n",
    "        else:\n",
    "            return group_value\n",
    "    \n",
    "    # âœ… ×©×œ×‘ 4: ×”×¤×¢×œ×ª ×”×¤×•× ×§×¦×™×” ×¢×œ ×›×œ ×”×©×•×¨×•×ª\n",
    "    dataset['total_floors'] = dataset.apply(smart_fill, axis=1)\n",
    "    \n",
    "    # âœ… ×©×œ×‘ 5: ×”×¡×¨×ª ×”×¢××•×“×” ×”×–×× ×™×ª\n",
    "    dataset.drop('_building_tax_bin', axis=1, inplace=True)\n",
    "\n",
    "  \n",
    "    \n",
    "########################################################################################################################################\n",
    "    # ×¢×™×‘×•×“ garden_area\n",
    "    def extract_garden_area(desc):\n",
    "        if pd.isna(desc):\n",
    "            return np.nan\n",
    "        desc = desc.replace(',', '')\n",
    "        match = re.search(r'×’×™× ×”[^0-9]{0,10}(\\d{1,4}) ?×\"×¨', desc)\n",
    "        if match:\n",
    "            try:\n",
    "                area = int(match.group(1))\n",
    "                if 1 <= area <= 300:\n",
    "                    return area\n",
    "            except:\n",
    "                return np.nan\n",
    "        return np.nan\n",
    "\n",
    "    dataset['garden_area_filled'] = dataset['garden_area']\n",
    "    missing_mask = dataset['garden_area'].isna()\n",
    "    dataset.loc[missing_mask, 'garden_area_filled'] = dataset.loc[missing_mask, 'description'].apply(extract_garden_area)\n",
    "\n",
    "    mask = (\n",
    "        dataset['garden_area_filled'].isna() &\n",
    "        (dataset['floor_clean'] > 3) &\n",
    "        (dataset['has_balcony'] == 0) &\n",
    "        (dataset['property_type_clean'].isin(['×“×™×¨×”', '×“×•×¤×œ×§×¡']))\n",
    "    )\n",
    "    dataset.loc[mask, 'garden_area'] = 0\n",
    "    dataset['garden_area_filled'] = dataset['garden_area_filled'].fillna(0)\n",
    "########################################################################################################################\n",
    "    def categorize_garden_area(x):\n",
    "        if pd.isna(x):\n",
    "            return '×œ× ×™×“×•×¢'\n",
    "        elif x == 0:\n",
    "            return '××™×Ÿ ×’×™× ×”'\n",
    "        elif x <= 20:\n",
    "            return '×§×˜× ×”'\n",
    "        elif x <= 50:\n",
    "            return '×‘×™× ×•× ×™×ª'\n",
    "        else:\n",
    "            return '×’×“×•×œ×”'\n",
    "\n",
    "    dataset['garden_size_category'] = dataset['garden_area'].apply(categorize_garden_area)\n",
    "#########################################################################################################################\n",
    "    # ×¢×™×‘×•×“ room_num\n",
    " # âœ… ×©×œ×‘ ××§×“×™×: ×”×—×œ×¤×ª ××¤×¡×™× ×‘-NaN\n",
    "    dataset['room_num'] = dataset['room_num'].replace(0, np.nan)\n",
    "    \n",
    "    # âœ… ×¤×•× ×§×¦×™×™×ª ×—×™×œ×•×¥ ××—×“×¨×™×\n",
    "    def extract_room_num_from_description(description):\n",
    "        if pd.isnull(description):\n",
    "            return np.nan\n",
    "    \n",
    "        description = str(description).strip()\n",
    "    \n",
    "        hebrew_numbers = {\n",
    "            '×—×“×¨ ××—×“': 1,\n",
    "            '×—×“×¨ ×•×—×¦×™': 1.5,\n",
    "            '×—×“×¨×™× ×•×—×¦×™': 1.5,\n",
    "            '×©× ×™×™×': 2,\n",
    "            '×©× ×™': 2,\n",
    "            '×©×ª×™': 2,\n",
    "            '×©×œ×•×©×”': 3,\n",
    "            '×©×œ×•×©': 3,\n",
    "            '××¨×‘×¢×”': 4,\n",
    "            '××¨×‘×¢': 4,\n",
    "            '×—××™×©×”': 5,\n",
    "            '×—××©': 5,\n",
    "            '×©×™×©×”': 6,\n",
    "            '×©×©': 6,\n",
    "            '×©×‘×¢×”': 7,\n",
    "            '×©×‘×¢': 7,\n",
    "            '×©××•× ×”': 8,\n",
    "            '×ª×©×¢×”': 9,\n",
    "            '×ª×©×¢': 9,\n",
    "            '×¢×©×¨×”': 10\n",
    "        }\n",
    "\n",
    "        # 1. ×ª×‘× ×™×•×ª ×›××• \"3 ×—×“×¨×™×\", \"2.5 ×—×“×¨×™×\"\n",
    "        match = re.search(r'(?:×“×™×¨×ª|×›×•×œ×œ×ª)?\\s*(\\d(?:[.,]\\d)?)\\s*×—×“×¨×™×?', description)\n",
    "        if match:\n",
    "            try:\n",
    "                return float(match.group(1).replace(',', '.'))\n",
    "            except:\n",
    "                return np.nan\n",
    "    \n",
    "        # 2. ××¡×¤×¨ ×œ×¤× ×™ \"×—×“×¨\"\n",
    "        match = re.search(r'(\\d(?:[.,]\\d)?)\\s*×—×“×¨', description)\n",
    "        if match:\n",
    "            try:\n",
    "                return float(match.group(1).replace(',', '.'))\n",
    "            except:\n",
    "                return np.nan\n",
    "    \n",
    "        # 3. ××™×¤×•×™ ×©×œ ××™×œ×™×\n",
    "        for word_form, number in hebrew_numbers.items():\n",
    "            if re.search(rf'{word_form}\\s*×—×“×¨', description):\n",
    "                return number\n",
    "    \n",
    "        # 4. fallback â€“ ×¨×§ \"×—×“×¨\"\n",
    "        if re.search(r'\\b×—×“×¨\\b', description):\n",
    "            return 1\n",
    "    \n",
    "        return np.nan\n",
    "    \n",
    "    # âœ… ×™×¦×™×¨×ª ×¢××•×“×” ×—×“×©×”\n",
    "    dataset['room_num_from_desc'] = dataset['description'].apply(extract_room_num_from_description)\n",
    "    \n",
    "    # âœ… ××™×œ×•×™ ×œ×¤×™ ×ª×™××•×¨\n",
    "    condition = (\n",
    "        dataset['room_num'].isna() &\n",
    "        dataset['room_num_from_desc'].notna() &\n",
    "        (dataset['room_num_from_desc'] != 0)\n",
    "    )\n",
    "    dataset.loc[condition, 'room_num'] = dataset.loc[condition, 'room_num_from_desc']\n",
    "    \n",
    "    # âœ… ××™× ×“×§×¦×™×” ×¢×œ ×©×•×¨×•×ª ×©×™×•×¡×¨×•\n",
    "    before_drop = len(dataset)\n",
    "    dataset.dropna(subset=['room_num'], inplace=True)\n",
    "    after_drop = len(dataset)\n",
    "    print(f\"âŒ ×”×•×¡×¨×• {before_drop - after_drop} ×©×•×¨×•×ª ×œ×œ× ×¢×¨×š ×‘-room_num ×œ××—×¨ × ×™×¡×™×•×Ÿ ×”××™×œ×•×™ ××ª×™××•×¨.\")\n",
    "\n",
    "################################################################################################################\n",
    "# ×¢×™×‘×•×“ area\n",
    "    Q1 = dataset['area'].quantile(0.25)\n",
    "    Q3 = dataset['area'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    mask_outliers = (dataset['area'] < 15) | (dataset['area'] > upper_bound)\n",
    "    dataset.loc[mask_outliers, 'area'] = np.nan\n",
    "\n",
    "    median_area_by_room = dataset.groupby('room_num')['area'].median()\n",
    "    dataset['area'] = dataset.apply(\n",
    "        lambda row: median_area_by_room[row['room_num']] if pd.isna(row['area']) else row['area'],\n",
    "        axis=1\n",
    "    )\n",
    "#################################################################################################################\n",
    "    # ×¢×™×‘×•×“ building_tax\n",
    "    Q1 = dataset['building_tax'].quantile(0.25)\n",
    "    Q3 = dataset['building_tax'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    lower_bound = 0\n",
    "\n",
    "    high_outliers_mask = dataset['building_tax'] > 1500\n",
    "    dataset.loc[high_outliers_mask, 'building_tax'] = np.nan\n",
    "\n",
    "    low_outliers_mask = (dataset['building_tax'] < 50) & (dataset['building_tax'] != 0)\n",
    "    dataset.loc[low_outliers_mask, 'building_tax'] = np.nan\n",
    "\n",
    "    mask = dataset['property_type_clean'] == '×¤×¨×˜×™'\n",
    "    dataset.loc[mask, 'building_tax'] = 0\n",
    "\n",
    "    def categorize_building_tax(value):\n",
    "        if pd.isna(value):\n",
    "            return \"×œ× ×™×•×“×¢\"\n",
    "        elif 0 <= value < 500:\n",
    "            return \"×–×•×œ\"\n",
    "        elif 500 <= value < 1000:\n",
    "            return \"×××•×¦×¢\"\n",
    "        elif 1000 <= value <= 1500:\n",
    "            return \"×’×‘×•×”\"\n",
    "        else:\n",
    "            return \"×œ× ×™×“×•×¢\"\n",
    "\n",
    "    dataset[\"building_tax_category\"] = dataset[\"building_tax\"].apply(categorize_building_tax)\n",
    "    dataset[\"building_tax_category\"] = dataset[\"building_tax_category\"].astype(\"category\")\n",
    "\n",
    "    def extract_building_tax(desc):\n",
    "        if pd.isna(desc):\n",
    "            return np.nan\n",
    "        desc = desc.replace(',', '')\n",
    "        patterns = [\n",
    "            r'××™×¡[×™|×™] [×”]?×‘× [×™×™]×Ÿ[^0-9]{0,10}(\\\\d{2,5})',\n",
    "            r'×•×¢×“[^0-9]{0,10}(\\\\d{2,5})',\n",
    "            r'×¢×“ ×‘× [×™×™]×Ÿ[^0-9]{0,10}(\\\\d{2,5})',\n",
    "        ]\n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, desc)\n",
    "            if match:\n",
    "                try:\n",
    "                    value = int(match.group(1))\n",
    "                    if 0 < value < 3000:\n",
    "                        return value\n",
    "                except:\n",
    "                    return np.nan\n",
    "        return np.nan\n",
    "\n",
    "    missing_mask = dataset['building_tax'].isna()\n",
    "    dataset.loc[missing_mask, 'building_tax'] = dataset.loc[missing_mask, 'description'].apply(extract_building_tax)\n",
    "\n",
    "    dataset['total_floors_round'] = dataset['total_floors'].round()\n",
    "    group1 = dataset.groupby(['has_parking', 'total_floors_round', 'has_safe_room'])['building_tax'].median()\n",
    "    group2 = dataset.groupby(['property_type_clean', 'neighborhood'])['building_tax'].median()\n",
    "\n",
    "    def fill_building_tax(row):\n",
    "        if pd.notna(row['building_tax']):\n",
    "            return row['building_tax']\n",
    "        try:\n",
    "            val = group1.loc[row['has_parking'], row['total_floors_round'], row['has_safe_room']]\n",
    "            if pd.notna(val):\n",
    "                return val\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            val = group2.loc[row['property_type_clean'], row['neighborhood']]\n",
    "            if pd.notna(val):\n",
    "                return val\n",
    "        except:\n",
    "            pass\n",
    "        return dataset['building_tax'].median()\n",
    "\n",
    "    dataset['building_tax'] = dataset.apply(fill_building_tax, axis=1)\n",
    "    dataset.drop('total_floors_round', axis=1, inplace=True)\n",
    "################################################################################################################################\n",
    "    # ×¢×™×‘×•×“ monthly_arnona\n",
    "    dataset.loc[dataset['monthly_arnona'] == 0, 'monthly_arnona'] = np.nan\n",
    "    dataset.loc[dataset['monthly_arnona'] < 100, 'monthly_arnona'] = np.nan\n",
    "    dataset.loc[dataset['monthly_arnona'] > 5000, 'monthly_arnona'] = np.nan\n",
    "\n",
    "    def fill_missing_arnona(row):\n",
    "        if pd.notna(row['monthly_arnona']):\n",
    "            return row['monthly_arnona']\n",
    "\n",
    "        area_min = row['area'] - 5\n",
    "        area_max = row['area'] + 5\n",
    "        rooms_min = row['room_num'] - 1\n",
    "        rooms_max = row['room_num'] + 1\n",
    "\n",
    "        similar = dataset[\n",
    "            (dataset['neighborhood'] == row['neighborhood']) &\n",
    "            (dataset['area'] >= area_min) & (dataset['area'] <= area_max) &\n",
    "            (dataset['room_num'] >= rooms_min) & (dataset['room_num'] <= rooms_max) &\n",
    "            (dataset['monthly_arnona'].notna())\n",
    "        ]\n",
    "\n",
    "        if not similar.empty:\n",
    "            return similar['monthly_arnona'].mean()\n",
    "        else:\n",
    "            return np.nan\n",
    "\n",
    "    dataset['monthly_arnona'] = dataset.apply(fill_missing_arnona, axis=1)\n",
    "\n",
    "    def fill_arnona_by_address_or_neighborhood(row):\n",
    "        if pd.notna(row['monthly_arnona']):\n",
    "            return row['monthly_arnona']\n",
    "\n",
    "        if pd.notna(row['street']):\n",
    "            street_mask = (dataset['street'] == row['street']) & dataset['monthly_arnona'].notna()\n",
    "            if street_mask.sum() >= 3:\n",
    "                return dataset.loc[street_mask, 'monthly_arnona'].median()\n",
    "\n",
    "        if pd.notna(row['neighborhood']):\n",
    "            neigh_mask = (dataset['neighborhood'] == row['neighborhood']) & dataset['monthly_arnona'].notna()\n",
    "            if neigh_mask.sum() >= 3:\n",
    "                return dataset.loc[neigh_mask, 'monthly_arnona'].median()\n",
    "\n",
    "        return np.nan\n",
    "\n",
    "    dataset['monthly_arnona'] = dataset.apply(fill_arnona_by_address_or_neighborhood, axis=1)\n",
    "\n",
    "    def fill_arnona_by_distance_similarity(row, median_distance_by_neigh, median_arnona_by_neigh, global_median):\n",
    "        if pd.notna(row['monthly_arnona']):\n",
    "            return row['monthly_arnona']\n",
    "\n",
    "        if pd.isna(row['distance_from_center']):\n",
    "            return global_median\n",
    "\n",
    "        diffs = (median_distance_by_neigh - row['distance_from_center']).abs()\n",
    "        closest_neigh = diffs.idxmin()\n",
    "        value = median_arnona_by_neigh.get(closest_neigh, np.nan)\n",
    "        if pd.isna(value):\n",
    "            return global_median\n",
    "        return value\n",
    "\n",
    "    median_distance_by_neigh = dataset.groupby('neighborhood')['distance_from_center'].median()\n",
    "    median_arnona_by_neigh = dataset.groupby('neighborhood')['monthly_arnona'].median()\n",
    "    global_median_arnona = dataset['monthly_arnona'].median()\n",
    "\n",
    "    dataset['monthly_arnona'] = dataset.apply(\n",
    "        lambda row: fill_arnona_by_distance_similarity(\n",
    "            row,\n",
    "            median_distance_by_neigh,\n",
    "            median_arnona_by_neigh,\n",
    "            global_median_arnona\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "#########################################################################################################################\n",
    "# ×¢×™×‘×•×“ entering_category\n",
    "    def map_days_to_category(val):\n",
    "        if pd.isna(val):\n",
    "            return 'unknown'\n",
    "        elif val == 0:\n",
    "            return 'immediate'\n",
    "        elif val == -1:\n",
    "            return 'flexible'\n",
    "        else:\n",
    "            return 'in_future'\n",
    "\n",
    "    dataset['entering_category'] = dataset['days_to_enter'].apply(map_days_to_category)\n",
    "##########################################################################################################################\n",
    "    # ×¢×™×‘×•×“ num_of_payments\n",
    "    dataset['num_of_payments'] = dataset['num_of_payments'].replace(0, np.nan)\n",
    "    dataset['num_of_payments'] = dataset['num_of_payments'].fillna(dataset['num_of_payments'].mode()[0])\n",
    "    \n",
    "    # num_of_images\n",
    "    dataset['num_of_images'] = dataset['num_of_images'].fillna(0)\n",
    "\n",
    "    #×¢×™×‘×•×“ distance_from_center\n",
    "    def fill_missing_distances_via_api(dataset, api_key, center_address=\"×›×™×›×¨ ×“×™×–× ×’×•×£, ×ª×œ ××‘×™×‘\"):\n",
    "        def format_address(addr):\n",
    "            if not isinstance(addr, str) or not addr.strip():\n",
    "                return None\n",
    "            if \"×ª×œ ××‘×™×‘\" not in addr:\n",
    "                addr += \", ×ª×œ ××‘×™×‘\"\n",
    "            return addr\n",
    "\n",
    "#         def get_distance(origin, destination, api_key):\n",
    "#             url = f\"https://routes.googleapis.com/directions/v2:computeRoutes\"\n",
    "#             headers = {\n",
    "#                 'Content-Type': 'application/json',\n",
    "#                 'X-Goog-Api-Key': api_key,\n",
    "#                 'X-Goog-FieldMask': 'routes.distanceMeters'\n",
    "#             }\n",
    "#             body = {\n",
    "#                 \"origin\": {\"address\": origin},\n",
    "#                 \"destination\": {\"address\": destination},\n",
    "#                 \"travelMode\": \"DRIVE\",\n",
    "#                 \"routingPreference\": \"TRAFFIC_AWARE\"\n",
    "#             }\n",
    "\n",
    "#             response = requests.post(url, headers=headers, json=body)\n",
    "#             if response.status_code == 200:\n",
    "#                 data = response.json()\n",
    "#                 if 'routes' in data and data['routes']:\n",
    "#                     return float(data['routes'][0]['distanceMeters'])\n",
    "#             return None\n",
    "\n",
    "#         missing = dataset[dataset['distance_from_center'].isna()].copy()\n",
    "\n",
    "#         for i, row in missing.iterrows():\n",
    "#             address = format_address(row.get('address')) or format_address(row.get('street'))\n",
    "#             if not address:\n",
    "#                 continue\n",
    "\n",
    "#             distance = get_distance(address, center_address, api_key)\n",
    "#             time.sleep(0.2)\n",
    "\n",
    "#             if distance is not None:\n",
    "#                 dataset.at[i, 'distance_from_center'] = distance\n",
    "\n",
    "#         return dataset\n",
    "#     api_key =getpass.getpass()\n",
    "#     dataset = fill_missing_distances_via_api(dataset, api_key=api_key)    \n",
    "#     dataset.dropna(subset=['distance_from_center'], inplace=True)\n",
    "      \n",
    "    # Fill missing values in 'distance_from_center' with the global median\n",
    "    median_distance = dataset['distance_from_center'].median()\n",
    "    dataset['distance_from_center'].fillna(median_distance, inplace=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return dataset  \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "653c674f-504b-47c1-a4f8-d9bb11b6f768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ ×”×•×¡×¨×• 2 ×©×•×¨×•×ª ×œ×œ× price\n",
      "âŒ ×”×•×¡×¨×• 53 ×©×•×¨×•×ª ×¢× ××—×™×¨ ××—×•×¥ ×œ×˜×•×•×— 1500â€“18500\n",
      "âŒ ×”×•×¡×¨×• 2 ×©×•×¨×•×ª ×©×‘×”×Ÿ ×’× neighborhood ×•×’× address ×—×¡×¨×™×\n",
      "âŒ ×”×•×¡×¨×• 0 ×©×•×¨×•×ª ×¢× 4 ×¢×¨×›×™× ××• ×¤×—×•×ª (×œ× 0 ×•×œ× NaN)\n",
      "âŒ ×”×•×¡×¨×• 1 ×©×•×¨×•×ª ×¢× ×”×ª×™×™×—×¡×•×ª ×œ'×§×•×¨×•×ª ×—×™×™×'\n",
      "âŒ ×”×•×¡×¨×• 11 ×©×•×¨×•×ª ×©×‘×”×Ÿ ×¡×•×’ ×”× ×›×¡ ×œ× ×–×•×”×” (×œ× ×™×“×•×¢)\n",
      "\n",
      "ğŸ“Š ×¡×š ×”×›×•×œ × ×©××¨×• 719 ×©×•×¨×•×ª ××ª×•×š 731 ×”××§×•×¨×™×•×ª.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ngr25\\AppData\\Local\\Temp\\ipykernel_12348\\488166565.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset.replace({'': np.nan, ' ': np.nan}, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ ×”×•×¡×¨×• 7 ×©×•×¨×•×ª ×œ×œ× ×¢×¨×š ×‘-room_num ×œ××—×¨ × ×™×¡×™×•×Ÿ ×”××™×œ×•×™ ××ª×™××•×¨.\n"
     ]
    }
   ],
   "source": [
    "my_data_test=prepare_data(my_data_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffc03630-c327-4b47-b09f-90ccfe9cd40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "luxury_types = ['×’×’/×¤× ×˜×”××•×–', '×“×•×¤×œ×§×¡']\n",
    "my_data_test['is_luxury_type'] = my_data_test['property_type'].isin(luxury_types).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92ca434f-565c-45af-9033-c4643bf4e1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "features=['property_type_clean', 'neighborhood', 'street', 'room_num', 'floor_clean', 'area',\n",
    "       'garden_area_filled', 'garden_size_category','entering_category', 'num_of_payments', 'monthly_arnona',\n",
    "       'building_tax', 'total_floors', 'has_parking',\n",
    "       'has_storage', 'elevator', 'ac', 'handicap', 'has_bars',\n",
    "       'has_safe_room', 'has_balcony', 'is_furnished', 'is_renovated',\n",
    "       'num_of_images', 'distance_from_center', 'price','building_tax_category' ]\n",
    "my_data_test=my_data_test[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea70e037-d12f-477b-b339-8b81a2a669c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š MAE: 1546.85 â‚ª\n",
      "ğŸ“‰ RMSE: 2184.83 â‚ª\n",
      "ğŸ“ˆ RÂ²: 0.6331\n",
      "\n",
      "ğŸŒŸ Top 5 Most Important Features:\n",
      "             feature  importance\n",
      "                area    0.446767\n",
      "        building_tax    0.107908\n",
      "      monthly_arnona    0.055803\n",
      "            room_num    0.047291\n",
      "distance_from_center    0.035921\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# ---------- Define selected features ----------\n",
    "features = ['property_type_clean', 'neighborhood', 'street', 'room_num', 'floor_clean', 'area',\n",
    "            'garden_area_filled', 'garden_size_category', 'entering_category', 'num_of_payments',\n",
    "            'monthly_arnona', 'building_tax', 'total_floors', 'has_parking', 'has_storage', 'elevator',\n",
    "            'ac', 'handicap', 'has_bars', 'has_safe_room', 'has_balcony', 'is_furnished', 'is_renovated',\n",
    "            'num_of_images', 'distance_from_center', 'price', 'building_tax_category']\n",
    "\n",
    "# ---------- Select data ----------\n",
    "df = my_data_test[features].copy()\n",
    "\n",
    "# ---------- Split into X (features) and y (target) ----------\n",
    "X = df.drop(\"price\", axis=1)\n",
    "y = df[\"price\"]\n",
    "\n",
    "# ---------- Identify column types ----------\n",
    "categorical_cols = X.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "numeric_cols = X.select_dtypes(include=[\"int64\", \"float64\", \"bool\"]).columns.tolist()\n",
    "\n",
    "# ---------- Define transformations for categorical columns ----------\n",
    "categorical_transformer = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))  # fixed here\n",
    "])\n",
    "\n",
    "# ---------- Define transformations for numeric columns ----------\n",
    "numeric_transformer = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", MinMaxScaler())\n",
    "])\n",
    "\n",
    "# ---------- Combine all transformations ----------\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", numeric_transformer, numeric_cols),\n",
    "    (\"cat\", categorical_transformer, categorical_cols)\n",
    "])\n",
    "\n",
    "# ---------- Create pipeline with preprocessing and Random Forest model ----------\n",
    "pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"regressor\", RandomForestRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# ---------- Split data into train/test sets ----------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ---------- Train the model ----------\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# ---------- Save the full pipeline as trained_model.pkl\n",
    "with open(\"trained_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(pipeline, f)\n",
    "\n",
    "# ---------- Make predictions ----------\n",
    "predictions = pipeline.predict(X_test)\n",
    "\n",
    "# ---------- Evaluate model performance ----------\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "r2 = r2_score(y_test, predictions)\n",
    "\n",
    "print(f\"ğŸ“Š MAE: {mae:.2f} â‚ª\")\n",
    "print(f\"ğŸ“‰ RMSE: {rmse:.2f} â‚ª\")\n",
    "print(f\"ğŸ“ˆ RÂ²: {r2:.4f}\")\n",
    "\n",
    "# ---------- Feature importance ----------\n",
    "feature_names = numeric_cols + list(\n",
    "    pipeline.named_steps['preprocessor']\n",
    "    .named_transformers_['cat']\n",
    "    .named_steps['encoder']\n",
    "    .get_feature_names_out(categorical_cols)\n",
    ")\n",
    "\n",
    "importances = pipeline.named_steps[\"regressor\"].feature_importances_\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    \"feature\": feature_names,\n",
    "    \"importance\": importances\n",
    "}).sort_values(by=\"importance\", ascending=False)\n",
    "\n",
    "top5 = importance_df.head(5)\n",
    "print(\"\\nğŸŒŸ Top 5 Most Important Features:\")\n",
    "print(top5.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372b0858-1411-4369-a845-8bf238ebcefd",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03e19e3b-7c97-4964-887b-5ce3366d2edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model:\n",
    "\n",
    "import pickle\n",
    "pickle.dump(pipeline, open(\"trained_model.pkl\", \"wb\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776f822f-33f3-4c50-9afa-1b428fff24a9",
   "metadata": {},
   "source": [
    "## loading a saved model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3eb81c6c-ec23-4c0a-a039-cd430c279c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = pickle.load(open(\"trained_model.pkl\",\"rb\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
