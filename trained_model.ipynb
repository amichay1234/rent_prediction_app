{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69b3cc2a-8831-4008-9891-085138b95c60",
   "metadata": {},
   "source": [
    "## Project: Rental Apartment Analysis Tel Aviv - Jaffa- Machine Learning – Part 3\n",
    "### Presented by:  \n",
    "### Amichay Nager -316225986 \n",
    "### Tair Mimon-322240615\n",
    "### github: https://github.com/amichay1234/project_part2-/blob/main/Machine%20Learning%20%E2%80%93%20Part%202%20Final.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c42198d-2c91-450e-b7ce-e7972e0c365e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "from sklearn.model_selection import cross_val_score, KFold,LeaveOneOut, LeavePOut, GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import mean_absolute_error ,mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, make_scorer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import pickle\n",
    "import requests\n",
    "import time\n",
    "import getpass\n",
    "import googlemaps\n",
    "import movecolumn as mc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d3a4201-ae1e-42d5-9176-f9bca41bb2e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>property_type</th>\n",
       "      <th>neighborhood</th>\n",
       "      <th>address</th>\n",
       "      <th>room_num</th>\n",
       "      <th>floor</th>\n",
       "      <th>area</th>\n",
       "      <th>garden_area</th>\n",
       "      <th>days_to_enter</th>\n",
       "      <th>num_of_payments</th>\n",
       "      <th>monthly_arnona</th>\n",
       "      <th>...</th>\n",
       "      <th>ac</th>\n",
       "      <th>handicap</th>\n",
       "      <th>has_bars</th>\n",
       "      <th>has_safe_room</th>\n",
       "      <th>has_balcony</th>\n",
       "      <th>is_furnished</th>\n",
       "      <th>is_renovated</th>\n",
       "      <th>num_of_images</th>\n",
       "      <th>distance_from_center</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>דירה</td>\n",
       "      <td>הצפון הישן החלק המרכזי</td>\n",
       "      <td>מהר\"ל 25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>71</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>467.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1005.0</td>\n",
       "      <td>10150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>דירה</td>\n",
       "      <td>הצפון הישן החלק המרכזי</td>\n",
       "      <td>ארלוזורוב 35</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>70</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>253.0</td>\n",
       "      <td>6600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>דירה</td>\n",
       "      <td>הצפון הישן החלק המרכזי</td>\n",
       "      <td>וורמיזה 5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1</td>\n",
       "      <td>65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>740.0</td>\n",
       "      <td>9000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>דירה</td>\n",
       "      <td>הצפון הישן החלק המרכזי</td>\n",
       "      <td>עמנואל הרומי 30</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1206.0</td>\n",
       "      <td>5800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>דירה</td>\n",
       "      <td>הצפון הישן החלק המרכזי</td>\n",
       "      <td>ארלוזורוב 50</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>70</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>7700.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  property_type            neighborhood          address  room_num floor  \\\n",
       "0          דירה  הצפון הישן החלק המרכזי         מהר\"ל 25       3.0     2   \n",
       "1          דירה  הצפון הישן החלק המרכזי     ארלוזורוב 35       3.0     1   \n",
       "2          דירה  הצפון הישן החלק המרכזי        וורמיזה 5       2.5     1   \n",
       "3          דירה  הצפון הישן החלק המרכזי  עמנואל הרומי 30       2.0     3   \n",
       "4          דירה  הצפון הישן החלק המרכזי     ארלוזורוב 50       3.0     1   \n",
       "\n",
       "   area  garden_area  days_to_enter  num_of_payments  monthly_arnona  ...  ac  \\\n",
       "0    71          NaN            0.0             12.0           467.0  ...   1   \n",
       "1    70          NaN            0.0             12.0           240.0  ...   1   \n",
       "2    65          NaN            NaN             12.0           400.0  ...   1   \n",
       "3    40          NaN            0.0             12.0           100.0  ...   0   \n",
       "4    70          NaN            0.0             11.0           250.0  ...   1   \n",
       "\n",
       "   handicap has_bars  has_safe_room  has_balcony  is_furnished  is_renovated  \\\n",
       "0         0        0              1            1             0             0   \n",
       "1         0        1              0            1             0             0   \n",
       "2         1        0              0            1             0             1   \n",
       "3         0        0              0            0             0             0   \n",
       "4         0        1              0            0             0             1   \n",
       "\n",
       "   num_of_images  distance_from_center    price  \n",
       "0            6.0                1005.0  10150.0  \n",
       "1            3.0                 253.0   6600.0  \n",
       "2            8.0                 740.0   9000.0  \n",
       "3            2.0                1206.0   5800.0  \n",
       "4            5.0                 255.0   7700.0  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the file\n",
    "file_name = r'https://raw.githubusercontent.com/amichay1234/project_part2-/refs/heads/main/train.csv'\n",
    "dataset = pd.read_csv(file_name)\n",
    "\n",
    "# New column order\n",
    "new_order = [\n",
    "    \"property_type\", \"neighborhood\", \"address\", \"room_num\", \"floor\", \"area\", \"garden_area\", \"days_to_enter\", \"num_of_payments\",\n",
    "    \"monthly_arnona\", \"building_tax\", \"total_floors\", \"description\", \"has_parking\", \"has_storage\", \"elevator\", \"ac\", \"handicap\", \"has_bars\",\n",
    "    \"has_safe_room\", \"has_balcony\", \"is_furnished\", \"is_renovated\", \"num_of_images\", \"distance_from_center\", \"price\"]\n",
    "\n",
    "# Change the order of the columns accordingly.\n",
    "dataset = dataset[new_order]\n",
    "\n",
    "# Displaying the first 5 lines\n",
    "dataset.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a1c2351-b578-4354-af11-07f0b0ae6b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 26\n",
      "Number of columns: 788\n"
     ]
    }
   ],
   "source": [
    "num_rows = dataset.shape[1]\n",
    "print(f\"Number of rows: {num_rows}\")\n",
    "\n",
    "num_columns = dataset.shape[0]\n",
    "print(f\"Number of columns: {num_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c57dd341-7118-4456-9261-2a7d54fc5b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data_test=dataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d75cd47-71ac-4ac9-91b0-73f1e35e3f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(dataset):\n",
    "\n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    if 'price' in dataset.columns:\n",
    "        before = len(dataset)\n",
    "        dataset.dropna(subset=['price'], inplace=True)\n",
    "        print(f\"❌ הוסרו {before - len(dataset)} שורות ללא price\")\n",
    "\n",
    "        # שלב 2 – שמירה על טווח מחירים סביר\n",
    "        before = len(dataset)\n",
    "        dataset = dataset[(dataset['price'] >= 1500) & (dataset['price'] <= 18500)]\n",
    "        print(f\"❌ הוסרו {before - len(dataset)} שורות עם מחיר מחוץ לטווח 1500–18500\")\n",
    "    else:\n",
    "        print(\"⚠️ עמודת price לא קיימת – דילוג על שלבי סינון לפי מחיר\")\n",
    "\n",
    "    # שלב 3 – המרה של תווים חסרי מידע ל־NaN\n",
    "    dataset.replace({'': np.nan, ' ': np.nan}, inplace=True)\n",
    "    \n",
    "    # שלב 4 – הסרת שורות בלי neighborhood ו־address גם יחד\n",
    "    before = len(dataset)\n",
    "    dataset = dataset[~(dataset[\"neighborhood\"].isna() & dataset[\"address\"].isna())].reset_index(drop=True)\n",
    "    print(f\"❌ הוסרו {before - len(dataset)} שורות שבהן גם neighborhood וגם address חסרים\")\n",
    "\n",
    "    # שלב 5 – הסרת שורות עם פחות מ־5 ערכים (שאינם 0 או NaN)\n",
    "    before = len(dataset)\n",
    "    dataset = dataset[dataset.apply(lambda row: (row != 0).sum() > 4, axis=1)]\n",
    "    print(f\"❌ הוסרו {before - len(dataset)} שורות עם 4 ערכים או פחות (לא 0 ולא NaN)\")\n",
    "    # סיכום כולל\n",
    "   # print(f\"\\n📊 סך הכול נשארו {len(dataset)} שורות מתוך {start_rows} המקוריות.\")\n",
    "\n",
    "#################################################################################################################################\n",
    "\n",
    "    # 🔹 שמירה על כמות שורות לפני עיבוד\n",
    "    start_rows = len(dataset)\n",
    "    \n",
    "    # 🔹 שלב 1: סיווג לפי property_type\n",
    "    def classify_property_type(val): \n",
    "        if not isinstance(val, str):\n",
    "            return np.nan\n",
    "        val_lower = val.lower()\n",
    "        if 'סאבלט' in val_lower:\n",
    "            return 'סאבלט'\n",
    "        elif 'פרטי' in val_lower or 'קוטג' in val_lower or 'דו משפחתי' in val_lower:\n",
    "            return 'פרטי'\n",
    "        elif 'סטודיו' in val_lower or 'לופט' in val_lower:\n",
    "            return 'סטודיו/לופט'\n",
    "        elif 'גג' in val_lower or 'פנטהאוז' in val_lower:\n",
    "            return 'גג/פנטהאוז'\n",
    "        elif 'דופלקס' in val_lower:\n",
    "            return 'דופלקס'\n",
    "        elif 'דירת גן' in val_lower:\n",
    "            return 'דירת גן'\n",
    "        elif 'דירה' in val_lower or 'דירת' in val_lower:\n",
    "            return 'דירה'\n",
    "        else:\n",
    "            return np.nan\n",
    "    \n",
    "    # 🔹 שלב 2: ניחוש לפי description\n",
    "    def guess_type_from_description(desc):\n",
    "        if pd.isna(desc):\n",
    "            return np.nan\n",
    "        desc = desc.lower()\n",
    "        non_apartment_keywords = [\n",
    "            r'משרד', r'משרדים', r'קליניקה', r'גלריה', r'סטודיו לאמן',\n",
    "            r'חלל עבודה', r'תלת פאזי', r'מחסן', r'סדנה', r'בית מלאכה', r'מסחרי'\n",
    "        ]\n",
    "        for pattern in non_apartment_keywords:\n",
    "            if re.search(rf'\\b{pattern}\\b', desc):\n",
    "                return np.nan\n",
    "        patterns = [\n",
    "            (r'\\bיחידת דיור\\b', 'יחידת דיור'),\n",
    "            (r'\\bסאבלט\\b', 'סאבלט'),\n",
    "            (r'\\b(פרטי|קוטג|דו משפחתי)\\b', 'פרטי'),\n",
    "            (r'\\b(סטודיו|לופט)\\b', 'סטודיו/לופט'),\n",
    "            (r'\\b(גג|פנטהאוז)\\b', 'גג/פנטהאוז'),\n",
    "            (r'\\bדופלקס\\b', 'דופלקס'),\n",
    "            (r'\\bדירת גן\\b', 'דירת גן'),\n",
    "            (r'\\b(דירה|דירת)\\b', 'דירה'),\n",
    "            (r'\\bחדר\\b', 'דירה'),\n",
    "            (r'\\bבית\\b', 'דירה'),\n",
    "        ]\n",
    "        for pattern, value in patterns:\n",
    "            if re.search(pattern, desc):\n",
    "                return value\n",
    "        return np.nan\n",
    "    \n",
    "    # 🔹 החלה על הדאטה\n",
    "    dataset['property_type_clean'] = dataset['property_type'].apply(classify_property_type)\n",
    "    \n",
    "    # 🔹 שלב השלמה לפי תיאור\n",
    "    missing_mask = dataset['property_type_clean'].isna()\n",
    "    dataset.loc[missing_mask, 'property_type_clean'] = dataset.loc[missing_mask, 'description'].apply(guess_type_from_description)\n",
    "    \n",
    "    # 🔹 מילוי \"לא ידוע\"\n",
    "    dataset['property_type_clean'] = dataset['property_type_clean'].fillna('לא ידוע')\n",
    "    \n",
    "    # 🔹 סינון שורות עם \"קורות חיים\"\n",
    "    before = len(dataset)\n",
    "    dataset = dataset[~dataset['description'].str.contains('קורות חיים', case=False, na=False)]\n",
    "    print(f\"❌ הוסרו {before - len(dataset)} שורות עם התייחסות ל'קורות חיים'\")\n",
    "    \n",
    "    # 🔹 הסרת שורות עם 'לא ידוע'\n",
    "    before = len(dataset)\n",
    "    dataset = dataset[dataset['property_type_clean'] != 'לא ידוע']\n",
    "    print(f\"❌ הוסרו {before - len(dataset)} שורות שבהן סוג הנכס לא זוהה (לא ידוע)\")\n",
    "    \n",
    "    # 🔹 סיכום\n",
    "    print(f\"\\n📊 סך הכול נשארו {len(dataset)} שורות מתוך {start_rows} המקוריות.\")\n",
    "\n",
    "####################################################################################################################################\n",
    "    def split_address(address):\n",
    "        if not isinstance(address, str):\n",
    "            return pd.Series([None, None])\n",
    "        match = re.search(r'(\\D+)\\s*(\\d+)?', address)\n",
    "        if match:\n",
    "            street = match.group(1).strip()\n",
    "            number = match.group(2) if match.group(2) else None\n",
    "            return pd.Series([street, number])\n",
    "        return pd.Series([None, None])\n",
    "\n",
    "    dataset[['street', 'house_number']] = dataset['address'].apply(split_address)\n",
    "    dataset.drop('house_number', axis=1, inplace=True)\n",
    "\n",
    "    conditions = (dataset['floor'].isna()) & (dataset['property_type'].isin(['פרטי']))\n",
    "    dataset.loc[conditions, 'floor'] = 0\n",
    "    dataset['street'] = dataset['street'].fillna(\"Unknown\")\n",
    "######################################################################################################################\n",
    "    def extract_floor_clean(val):\n",
    "        if pd.isna(val):\n",
    "            return np.nan\n",
    "        val = str(val).strip()\n",
    "        if val == 'קרקע מתוך קרקע':\n",
    "            return 0\n",
    "        if 'קרקע' in val:\n",
    "            return 0\n",
    "        if 'מרתף' in val:\n",
    "            return -1\n",
    "        match = re.findall(r'\\d+', val)\n",
    "        if match:\n",
    "            try:\n",
    "                num = int(match[0])\n",
    "                if 0 <= num <= 100:\n",
    "                    return num\n",
    "            except:\n",
    "                return np.nan\n",
    "        return np.nan\n",
    "\n",
    "    dataset['floor_clean'] = dataset.apply(lambda row: extract_floor_clean(row['floor']), axis=1)\n",
    "    dataset['floor_clean'] = pd.to_numeric(dataset['floor_clean'], errors='coerce')\n",
    "    condition = (dataset['total_floors'] == 0) & (dataset['floor_clean'].isin([0, 1]))\n",
    "    dataset.loc[condition, 'total_floors'] = 1\n",
    "\n",
    "    def extract_floor_strict(val, total_floors):\n",
    "        if pd.isnull(val):\n",
    "            return np.nan\n",
    "        val = str(val).strip()\n",
    "        if val == '0' or 'קרקע' in val:\n",
    "            return 1\n",
    "        match = re.search(r'\\d+', val)\n",
    "        if not match:\n",
    "            return np.nan\n",
    "        num_str = match.group()\n",
    "        try:\n",
    "            num = int(num_str)\n",
    "        except:\n",
    "            return np.nan\n",
    "        if 0 < num <= 100:\n",
    "            return num\n",
    "        if not pd.isnull(total_floors):\n",
    "            tf_str = str(int(total_floors))\n",
    "            if num_str.endswith(tf_str):\n",
    "                floor_part = num_str[:-len(tf_str)]\n",
    "                if floor_part.isdigit():\n",
    "                    floor_val = int(floor_part)\n",
    "                    if 0 < floor_val <= 100:\n",
    "                        return floor_val\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "    dataset['floor_clean'] = dataset.apply(\n",
    "        lambda row: extract_floor_strict(row['floor'], row['total_floors']),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "# פונקציה לחילוץ מספר קומה מהתיאור\n",
    "    def extract_floor_from_description(desc):\n",
    "        if pd.isna(desc):\n",
    "            return np.nan\n",
    "        desc = desc.lower()\n",
    "\n",
    "        # קומה מספרית רגילה: \"קומה 3\", \"בקומה 10\", \"קומה: 5\"\n",
    "        match = re.search(r'קומ[ה|ת] (?:מס\\' )?(\\d{1,2})', desc)\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "\n",
    "        # קומה מעל משהו – לדוגמה \"מעל קומה 20\"\n",
    "        match = re.search(r'מעל.*?(\\d{1,2})', desc)\n",
    "        if match:\n",
    "            return int(match.group(1)) + 1\n",
    "\n",
    "        # קומת קרקע\n",
    "        if 'קומת קרקע' in desc or 'קרקע' in desc:\n",
    "            return 0\n",
    "\n",
    "        return np.nan\n",
    "\n",
    "    # שמירת עותק של העמודה לפני השינויים\n",
    "    original_floor = dataset['floor_clean'].copy()\n",
    "\n",
    "    # סינון שורות עם ערך חסר בקומה\n",
    "    missing_floor_mask = dataset['floor_clean'].isna()\n",
    "    dataset.loc[missing_floor_mask, 'floor_clean'] = dataset.loc[missing_floor_mask, 'description'].apply(extract_floor_from_description)\n",
    "\n",
    "    \n",
    "    dataset.dropna(subset=['floor_clean'], inplace=True) #1\n",
    "\n",
    "\n",
    "    def fix_merged_floor(row):\n",
    "        floor = row['floor_clean']\n",
    "        total = row['total_floors']\n",
    "        if pd.notna(floor) and pd.notna(total):\n",
    "            if floor > total:\n",
    "                digits = [int(d) for d in str(int(floor)) if d.isdigit()]\n",
    "                if int(total) in digits:\n",
    "                    digits.remove(int(total))\n",
    "                    if digits:\n",
    "                        new_floor = digits[0]\n",
    "                        if new_floor <= total:\n",
    "                            return new_floor\n",
    "        return floor\n",
    "\n",
    "    dataset['floor_clean'] = dataset.apply(fix_merged_floor, axis=1)\n",
    "\n",
    "###############################################################################################################\n",
    "    mask = (\n",
    "        (dataset['floor_clean'] > dataset['total_floors']) &\n",
    "        (dataset['floor_clean'].notna()) &\n",
    "        (dataset['total_floors'].notna())\n",
    "    )\n",
    "    dataset.loc[mask, 'total_floors'] = np.nan\n",
    "\n",
    "    def extract_total_floors_from_floor(val):\n",
    "        if pd.isna(val):\n",
    "            return np.nan\n",
    "        val = str(val).strip()\n",
    "        if val == 'קרקע מתוך קרקע':\n",
    "            return 1\n",
    "        if 'קרקע מתוך' in val:\n",
    "            match = re.search(r'קרקע מתוך (\\d+)', val)\n",
    "            if match:\n",
    "                return int(match.group(1))\n",
    "            else:\n",
    "                return np.nan\n",
    "        match = re.findall(r'\\d+', val)\n",
    "        if len(match) >= 2:\n",
    "            return int(match[-1])\n",
    "        return np.nan\n",
    "\n",
    "    mask_missing = dataset['total_floors'].isna()\n",
    "    dataset.loc[mask_missing, 'total_floors'] = dataset.loc[mask_missing, 'floor'].apply(extract_total_floors_from_floor)\n",
    "    dataset['total_floors'] = dataset['total_floors'].astype(float)\n",
    "\n",
    "    # ✅ שלב 1: יצירת קטגוריות זמניות של מיסי בניין\n",
    "    dataset['_building_tax_bin'] = pd.qcut(dataset['building_tax'], q=4, duplicates='drop')\n",
    "    \n",
    "    # ✅ שלב 2: חישוב חציון total_floors לפי elevator + has_parking + tax_bin\n",
    "    group_median = dataset.groupby(['elevator', 'has_parking', '_building_tax_bin'])['total_floors'].median()\n",
    "    \n",
    "    # ✅ שלב 3: פונקציית מילוי חכמה שמתחשבת גם בקומה בפועל\n",
    "    def smart_fill(row):\n",
    "        if pd.notna(row['total_floors']):\n",
    "            return row['total_floors']\n",
    "    \n",
    "        try:\n",
    "            group_value = group_median.loc[row['elevator'], row['has_parking'], row['_building_tax_bin']]\n",
    "        except:\n",
    "            group_value = dataset['total_floors'].median()\n",
    "    \n",
    "        if pd.notna(row.get('floor_clean')):\n",
    "            return max(group_value, row['floor_clean'])\n",
    "        else:\n",
    "            return group_value\n",
    "    \n",
    "    # ✅ שלב 4: הפעלת הפונקציה על כל השורות\n",
    "    dataset['total_floors'] = dataset.apply(smart_fill, axis=1)\n",
    "    \n",
    "    # ✅ שלב 5: הסרת העמודה הזמנית\n",
    "    dataset.drop('_building_tax_bin', axis=1, inplace=True)\n",
    "\n",
    "  \n",
    "    \n",
    "########################################################################################################################################\n",
    "    # עיבוד garden_area\n",
    "    def extract_garden_area(desc):\n",
    "        if pd.isna(desc):\n",
    "            return np.nan\n",
    "        desc = desc.replace(',', '')\n",
    "        match = re.search(r'גינה[^0-9]{0,10}(\\d{1,4}) ?מ\"ר', desc)\n",
    "        if match:\n",
    "            try:\n",
    "                area = int(match.group(1))\n",
    "                if 1 <= area <= 300:\n",
    "                    return area\n",
    "            except:\n",
    "                return np.nan\n",
    "        return np.nan\n",
    "\n",
    "    dataset['garden_area_filled'] = dataset['garden_area']\n",
    "    missing_mask = dataset['garden_area'].isna()\n",
    "    dataset.loc[missing_mask, 'garden_area_filled'] = dataset.loc[missing_mask, 'description'].apply(extract_garden_area)\n",
    "\n",
    "    mask = (\n",
    "        dataset['garden_area_filled'].isna() &\n",
    "        (dataset['floor_clean'] > 3) &\n",
    "        (dataset['has_balcony'] == 0) &\n",
    "        (dataset['property_type_clean'].isin(['דירה', 'דופלקס']))\n",
    "    )\n",
    "    dataset.loc[mask, 'garden_area'] = 0\n",
    "    dataset['garden_area_filled'] = dataset['garden_area_filled'].fillna(0)\n",
    "########################################################################################################################\n",
    "    def categorize_garden_area(x):\n",
    "        if pd.isna(x):\n",
    "            return 'לא ידוע'\n",
    "        elif x == 0:\n",
    "            return 'אין גינה'\n",
    "        elif x <= 20:\n",
    "            return 'קטנה'\n",
    "        elif x <= 50:\n",
    "            return 'בינונית'\n",
    "        else:\n",
    "            return 'גדולה'\n",
    "\n",
    "    dataset['garden_size_category'] = dataset['garden_area'].apply(categorize_garden_area)\n",
    "#########################################################################################################################\n",
    "    # עיבוד room_num\n",
    " # ✅ שלב מקדים: החלפת אפסים ב-NaN\n",
    "    dataset['room_num'] = dataset['room_num'].replace(0, np.nan)\n",
    "    \n",
    "    # ✅ פונקציית חילוץ מחדרים\n",
    "    def extract_room_num_from_description(description):\n",
    "        if pd.isnull(description):\n",
    "            return np.nan\n",
    "    \n",
    "        description = str(description).strip()\n",
    "    \n",
    "        hebrew_numbers = {\n",
    "            'חדר אחד': 1,\n",
    "            'חדר וחצי': 1.5,\n",
    "            'חדרים וחצי': 1.5,\n",
    "            'שניים': 2,\n",
    "            'שני': 2,\n",
    "            'שתי': 2,\n",
    "            'שלושה': 3,\n",
    "            'שלוש': 3,\n",
    "            'ארבעה': 4,\n",
    "            'ארבע': 4,\n",
    "            'חמישה': 5,\n",
    "            'חמש': 5,\n",
    "            'שישה': 6,\n",
    "            'שש': 6,\n",
    "            'שבעה': 7,\n",
    "            'שבע': 7,\n",
    "            'שמונה': 8,\n",
    "            'תשעה': 9,\n",
    "            'תשע': 9,\n",
    "            'עשרה': 10\n",
    "        }\n",
    "\n",
    "        # 1. תבניות כמו \"3 חדרים\", \"2.5 חדרים\"\n",
    "        match = re.search(r'(?:דירת|כוללת)?\\s*(\\d(?:[.,]\\d)?)\\s*חדרים?', description)\n",
    "        if match:\n",
    "            try:\n",
    "                return float(match.group(1).replace(',', '.'))\n",
    "            except:\n",
    "                return np.nan\n",
    "    \n",
    "        # 2. מספר לפני \"חדר\"\n",
    "        match = re.search(r'(\\d(?:[.,]\\d)?)\\s*חדר', description)\n",
    "        if match:\n",
    "            try:\n",
    "                return float(match.group(1).replace(',', '.'))\n",
    "            except:\n",
    "                return np.nan\n",
    "    \n",
    "        # 3. מיפוי של מילים\n",
    "        for word_form, number in hebrew_numbers.items():\n",
    "            if re.search(rf'{word_form}\\s*חדר', description):\n",
    "                return number\n",
    "    \n",
    "        # 4. fallback – רק \"חדר\"\n",
    "        if re.search(r'\\bחדר\\b', description):\n",
    "            return 1\n",
    "    \n",
    "        return np.nan\n",
    "    \n",
    "    # ✅ יצירת עמודה חדשה\n",
    "    dataset['room_num_from_desc'] = dataset['description'].apply(extract_room_num_from_description)\n",
    "    \n",
    "    # ✅ מילוי לפי תיאור\n",
    "    condition = (\n",
    "        dataset['room_num'].isna() &\n",
    "        dataset['room_num_from_desc'].notna() &\n",
    "        (dataset['room_num_from_desc'] != 0)\n",
    "    )\n",
    "    dataset.loc[condition, 'room_num'] = dataset.loc[condition, 'room_num_from_desc']\n",
    "    \n",
    "    # ✅ אינדקציה על שורות שיוסרו\n",
    "    before_drop = len(dataset)\n",
    "    dataset.dropna(subset=['room_num'], inplace=True)\n",
    "    after_drop = len(dataset)\n",
    "    print(f\"❌ הוסרו {before_drop - after_drop} שורות ללא ערך ב-room_num לאחר ניסיון המילוי מתיאור.\")\n",
    "\n",
    "################################################################################################################\n",
    "# עיבוד area\n",
    "    Q1 = dataset['area'].quantile(0.25)\n",
    "    Q3 = dataset['area'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    mask_outliers = (dataset['area'] < 15) | (dataset['area'] > upper_bound)\n",
    "    dataset.loc[mask_outliers, 'area'] = np.nan\n",
    "\n",
    "    median_area_by_room = dataset.groupby('room_num')['area'].median()\n",
    "    dataset['area'] = dataset.apply(\n",
    "        lambda row: median_area_by_room[row['room_num']] if pd.isna(row['area']) else row['area'],\n",
    "        axis=1\n",
    "    )\n",
    "#################################################################################################################\n",
    "    # עיבוד building_tax\n",
    "    Q1 = dataset['building_tax'].quantile(0.25)\n",
    "    Q3 = dataset['building_tax'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    lower_bound = 0\n",
    "\n",
    "    high_outliers_mask = dataset['building_tax'] > 1500\n",
    "    dataset.loc[high_outliers_mask, 'building_tax'] = np.nan\n",
    "\n",
    "    low_outliers_mask = (dataset['building_tax'] < 50) & (dataset['building_tax'] != 0)\n",
    "    dataset.loc[low_outliers_mask, 'building_tax'] = np.nan\n",
    "\n",
    "    mask = dataset['property_type_clean'] == 'פרטי'\n",
    "    dataset.loc[mask, 'building_tax'] = 0\n",
    "\n",
    "    def categorize_building_tax(value):\n",
    "        if pd.isna(value):\n",
    "            return \"לא יודע\"\n",
    "        elif 0 <= value < 500:\n",
    "            return \"זול\"\n",
    "        elif 500 <= value < 1000:\n",
    "            return \"ממוצע\"\n",
    "        elif 1000 <= value <= 1500:\n",
    "            return \"גבוה\"\n",
    "        else:\n",
    "            return \"לא ידוע\"\n",
    "\n",
    "    dataset[\"building_tax_category\"] = dataset[\"building_tax\"].apply(categorize_building_tax)\n",
    "    dataset[\"building_tax_category\"] = dataset[\"building_tax_category\"].astype(\"category\")\n",
    "\n",
    "    def extract_building_tax(desc):\n",
    "        if pd.isna(desc):\n",
    "            return np.nan\n",
    "        desc = desc.replace(',', '')\n",
    "        patterns = [\n",
    "            r'מיס[י|י] [ה]?בנ[יי]ן[^0-9]{0,10}(\\\\d{2,5})',\n",
    "            r'ועד[^0-9]{0,10}(\\\\d{2,5})',\n",
    "            r'עד בנ[יי]ן[^0-9]{0,10}(\\\\d{2,5})',\n",
    "        ]\n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, desc)\n",
    "            if match:\n",
    "                try:\n",
    "                    value = int(match.group(1))\n",
    "                    if 0 < value < 3000:\n",
    "                        return value\n",
    "                except:\n",
    "                    return np.nan\n",
    "        return np.nan\n",
    "\n",
    "    missing_mask = dataset['building_tax'].isna()\n",
    "    dataset.loc[missing_mask, 'building_tax'] = dataset.loc[missing_mask, 'description'].apply(extract_building_tax)\n",
    "\n",
    "    dataset['total_floors_round'] = dataset['total_floors'].round()\n",
    "    group1 = dataset.groupby(['has_parking', 'total_floors_round', 'has_safe_room'])['building_tax'].median()\n",
    "    group2 = dataset.groupby(['property_type_clean', 'neighborhood'])['building_tax'].median()\n",
    "\n",
    "    def fill_building_tax(row):\n",
    "        if pd.notna(row['building_tax']):\n",
    "            return row['building_tax']\n",
    "        try:\n",
    "            val = group1.loc[row['has_parking'], row['total_floors_round'], row['has_safe_room']]\n",
    "            if pd.notna(val):\n",
    "                return val\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            val = group2.loc[row['property_type_clean'], row['neighborhood']]\n",
    "            if pd.notna(val):\n",
    "                return val\n",
    "        except:\n",
    "            pass\n",
    "        return dataset['building_tax'].median()\n",
    "\n",
    "    dataset['building_tax'] = dataset.apply(fill_building_tax, axis=1)\n",
    "    dataset.drop('total_floors_round', axis=1, inplace=True)\n",
    "################################################################################################################################\n",
    "    # עיבוד monthly_arnona\n",
    "    dataset.loc[dataset['monthly_arnona'] == 0, 'monthly_arnona'] = np.nan\n",
    "    dataset.loc[dataset['monthly_arnona'] < 100, 'monthly_arnona'] = np.nan\n",
    "    dataset.loc[dataset['monthly_arnona'] > 5000, 'monthly_arnona'] = np.nan\n",
    "\n",
    "    def fill_missing_arnona(row):\n",
    "        if pd.notna(row['monthly_arnona']):\n",
    "            return row['monthly_arnona']\n",
    "\n",
    "        area_min = row['area'] - 5\n",
    "        area_max = row['area'] + 5\n",
    "        rooms_min = row['room_num'] - 1\n",
    "        rooms_max = row['room_num'] + 1\n",
    "\n",
    "        similar = dataset[\n",
    "            (dataset['neighborhood'] == row['neighborhood']) &\n",
    "            (dataset['area'] >= area_min) & (dataset['area'] <= area_max) &\n",
    "            (dataset['room_num'] >= rooms_min) & (dataset['room_num'] <= rooms_max) &\n",
    "            (dataset['monthly_arnona'].notna())\n",
    "        ]\n",
    "\n",
    "        if not similar.empty:\n",
    "            return similar['monthly_arnona'].mean()\n",
    "        else:\n",
    "            return np.nan\n",
    "\n",
    "    dataset['monthly_arnona'] = dataset.apply(fill_missing_arnona, axis=1)\n",
    "\n",
    "    def fill_arnona_by_address_or_neighborhood(row):\n",
    "        if pd.notna(row['monthly_arnona']):\n",
    "            return row['monthly_arnona']\n",
    "\n",
    "        if pd.notna(row['street']):\n",
    "            street_mask = (dataset['street'] == row['street']) & dataset['monthly_arnona'].notna()\n",
    "            if street_mask.sum() >= 3:\n",
    "                return dataset.loc[street_mask, 'monthly_arnona'].median()\n",
    "\n",
    "        if pd.notna(row['neighborhood']):\n",
    "            neigh_mask = (dataset['neighborhood'] == row['neighborhood']) & dataset['monthly_arnona'].notna()\n",
    "            if neigh_mask.sum() >= 3:\n",
    "                return dataset.loc[neigh_mask, 'monthly_arnona'].median()\n",
    "\n",
    "        return np.nan\n",
    "\n",
    "    dataset['monthly_arnona'] = dataset.apply(fill_arnona_by_address_or_neighborhood, axis=1)\n",
    "\n",
    "    def fill_arnona_by_distance_similarity(row, median_distance_by_neigh, median_arnona_by_neigh, global_median):\n",
    "        if pd.notna(row['monthly_arnona']):\n",
    "            return row['monthly_arnona']\n",
    "\n",
    "        if pd.isna(row['distance_from_center']):\n",
    "            return global_median\n",
    "\n",
    "        diffs = (median_distance_by_neigh - row['distance_from_center']).abs()\n",
    "        closest_neigh = diffs.idxmin()\n",
    "        value = median_arnona_by_neigh.get(closest_neigh, np.nan)\n",
    "        if pd.isna(value):\n",
    "            return global_median\n",
    "        return value\n",
    "\n",
    "    median_distance_by_neigh = dataset.groupby('neighborhood')['distance_from_center'].median()\n",
    "    median_arnona_by_neigh = dataset.groupby('neighborhood')['monthly_arnona'].median()\n",
    "    global_median_arnona = dataset['monthly_arnona'].median()\n",
    "\n",
    "    dataset['monthly_arnona'] = dataset.apply(\n",
    "        lambda row: fill_arnona_by_distance_similarity(\n",
    "            row,\n",
    "            median_distance_by_neigh,\n",
    "            median_arnona_by_neigh,\n",
    "            global_median_arnona\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "#########################################################################################################################\n",
    "# עיבוד entering_category\n",
    "    def map_days_to_category(val):\n",
    "        if pd.isna(val):\n",
    "            return 'unknown'\n",
    "        elif val == 0:\n",
    "            return 'immediate'\n",
    "        elif val == -1:\n",
    "            return 'flexible'\n",
    "        else:\n",
    "            return 'in_future'\n",
    "\n",
    "    dataset['entering_category'] = dataset['days_to_enter'].apply(map_days_to_category)\n",
    "##########################################################################################################################\n",
    "    # עיבוד num_of_payments\n",
    "    dataset['num_of_payments'] = dataset['num_of_payments'].replace(0, np.nan)\n",
    "    dataset['num_of_payments'] = dataset['num_of_payments'].fillna(dataset['num_of_payments'].mode()[0])\n",
    "    \n",
    "    # num_of_images\n",
    "    dataset['num_of_images'] = dataset['num_of_images'].fillna(0)\n",
    "\n",
    "    #עיבוד distance_from_center\n",
    "    def fill_missing_distances_via_api(dataset, api_key, center_address=\"כיכר דיזנגוף, תל אביב\"):\n",
    "        def format_address(addr):\n",
    "            if not isinstance(addr, str) or not addr.strip():\n",
    "                return None\n",
    "            if \"תל אביב\" not in addr:\n",
    "                addr += \", תל אביב\"\n",
    "            return addr\n",
    "\n",
    "#         def get_distance(origin, destination, api_key):\n",
    "#             url = f\"https://routes.googleapis.com/directions/v2:computeRoutes\"\n",
    "#             headers = {\n",
    "#                 'Content-Type': 'application/json',\n",
    "#                 'X-Goog-Api-Key': api_key,\n",
    "#                 'X-Goog-FieldMask': 'routes.distanceMeters'\n",
    "#             }\n",
    "#             body = {\n",
    "#                 \"origin\": {\"address\": origin},\n",
    "#                 \"destination\": {\"address\": destination},\n",
    "#                 \"travelMode\": \"DRIVE\",\n",
    "#                 \"routingPreference\": \"TRAFFIC_AWARE\"\n",
    "#             }\n",
    "\n",
    "#             response = requests.post(url, headers=headers, json=body)\n",
    "#             if response.status_code == 200:\n",
    "#                 data = response.json()\n",
    "#                 if 'routes' in data and data['routes']:\n",
    "#                     return float(data['routes'][0]['distanceMeters'])\n",
    "#             return None\n",
    "\n",
    "#         missing = dataset[dataset['distance_from_center'].isna()].copy()\n",
    "\n",
    "#         for i, row in missing.iterrows():\n",
    "#             address = format_address(row.get('address')) or format_address(row.get('street'))\n",
    "#             if not address:\n",
    "#                 continue\n",
    "\n",
    "#             distance = get_distance(address, center_address, api_key)\n",
    "#             time.sleep(0.2)\n",
    "\n",
    "#             if distance is not None:\n",
    "#                 dataset.at[i, 'distance_from_center'] = distance\n",
    "\n",
    "#         return dataset\n",
    "#     api_key =getpass.getpass()\n",
    "#     dataset = fill_missing_distances_via_api(dataset, api_key=api_key)    \n",
    "#     dataset.dropna(subset=['distance_from_center'], inplace=True)\n",
    "      \n",
    "    # Fill missing values in 'distance_from_center' with the global median\n",
    "    median_distance = dataset['distance_from_center'].median()\n",
    "    dataset['distance_from_center'].fillna(median_distance, inplace=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return dataset  \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "653c674f-504b-47c1-a4f8-d9bb11b6f768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ הוסרו 2 שורות ללא price\n",
      "❌ הוסרו 53 שורות עם מחיר מחוץ לטווח 1500–18500\n",
      "❌ הוסרו 2 שורות שבהן גם neighborhood וגם address חסרים\n",
      "❌ הוסרו 0 שורות עם 4 ערכים או פחות (לא 0 ולא NaN)\n",
      "❌ הוסרו 1 שורות עם התייחסות ל'קורות חיים'\n",
      "❌ הוסרו 11 שורות שבהן סוג הנכס לא זוהה (לא ידוע)\n",
      "\n",
      "📊 סך הכול נשארו 719 שורות מתוך 731 המקוריות.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ngr25\\AppData\\Local\\Temp\\ipykernel_12348\\488166565.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset.replace({'': np.nan, ' ': np.nan}, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ הוסרו 7 שורות ללא ערך ב-room_num לאחר ניסיון המילוי מתיאור.\n"
     ]
    }
   ],
   "source": [
    "my_data_test=prepare_data(my_data_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffc03630-c327-4b47-b09f-90ccfe9cd40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "luxury_types = ['גג/פנטהאוז', 'דופלקס']\n",
    "my_data_test['is_luxury_type'] = my_data_test['property_type'].isin(luxury_types).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92ca434f-565c-45af-9033-c4643bf4e1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "features=['property_type_clean', 'neighborhood', 'street', 'room_num', 'floor_clean', 'area',\n",
    "       'garden_area_filled', 'garden_size_category','entering_category', 'num_of_payments', 'monthly_arnona',\n",
    "       'building_tax', 'total_floors', 'has_parking',\n",
    "       'has_storage', 'elevator', 'ac', 'handicap', 'has_bars',\n",
    "       'has_safe_room', 'has_balcony', 'is_furnished', 'is_renovated',\n",
    "       'num_of_images', 'distance_from_center', 'price','building_tax_category' ]\n",
    "my_data_test=my_data_test[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea70e037-d12f-477b-b339-8b81a2a669c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 MAE: 1546.85 ₪\n",
      "📉 RMSE: 2184.83 ₪\n",
      "📈 R²: 0.6331\n",
      "\n",
      "🌟 Top 5 Most Important Features:\n",
      "             feature  importance\n",
      "                area    0.446767\n",
      "        building_tax    0.107908\n",
      "      monthly_arnona    0.055803\n",
      "            room_num    0.047291\n",
      "distance_from_center    0.035921\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# ---------- Define selected features ----------\n",
    "features = ['property_type_clean', 'neighborhood', 'street', 'room_num', 'floor_clean', 'area',\n",
    "            'garden_area_filled', 'garden_size_category', 'entering_category', 'num_of_payments',\n",
    "            'monthly_arnona', 'building_tax', 'total_floors', 'has_parking', 'has_storage', 'elevator',\n",
    "            'ac', 'handicap', 'has_bars', 'has_safe_room', 'has_balcony', 'is_furnished', 'is_renovated',\n",
    "            'num_of_images', 'distance_from_center', 'price', 'building_tax_category']\n",
    "\n",
    "# ---------- Select data ----------\n",
    "df = my_data_test[features].copy()\n",
    "\n",
    "# ---------- Split into X (features) and y (target) ----------\n",
    "X = df.drop(\"price\", axis=1)\n",
    "y = df[\"price\"]\n",
    "\n",
    "# ---------- Identify column types ----------\n",
    "categorical_cols = X.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "numeric_cols = X.select_dtypes(include=[\"int64\", \"float64\", \"bool\"]).columns.tolist()\n",
    "\n",
    "# ---------- Define transformations for categorical columns ----------\n",
    "categorical_transformer = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))  # fixed here\n",
    "])\n",
    "\n",
    "# ---------- Define transformations for numeric columns ----------\n",
    "numeric_transformer = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", MinMaxScaler())\n",
    "])\n",
    "\n",
    "# ---------- Combine all transformations ----------\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", numeric_transformer, numeric_cols),\n",
    "    (\"cat\", categorical_transformer, categorical_cols)\n",
    "])\n",
    "\n",
    "# ---------- Create pipeline with preprocessing and Random Forest model ----------\n",
    "pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"regressor\", RandomForestRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# ---------- Split data into train/test sets ----------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ---------- Train the model ----------\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# ---------- Save the full pipeline as trained_model.pkl\n",
    "with open(\"trained_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(pipeline, f)\n",
    "\n",
    "# ---------- Make predictions ----------\n",
    "predictions = pipeline.predict(X_test)\n",
    "\n",
    "# ---------- Evaluate model performance ----------\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "r2 = r2_score(y_test, predictions)\n",
    "\n",
    "print(f\"📊 MAE: {mae:.2f} ₪\")\n",
    "print(f\"📉 RMSE: {rmse:.2f} ₪\")\n",
    "print(f\"📈 R²: {r2:.4f}\")\n",
    "\n",
    "# ---------- Feature importance ----------\n",
    "feature_names = numeric_cols + list(\n",
    "    pipeline.named_steps['preprocessor']\n",
    "    .named_transformers_['cat']\n",
    "    .named_steps['encoder']\n",
    "    .get_feature_names_out(categorical_cols)\n",
    ")\n",
    "\n",
    "importances = pipeline.named_steps[\"regressor\"].feature_importances_\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    \"feature\": feature_names,\n",
    "    \"importance\": importances\n",
    "}).sort_values(by=\"importance\", ascending=False)\n",
    "\n",
    "top5 = importance_df.head(5)\n",
    "print(\"\\n🌟 Top 5 Most Important Features:\")\n",
    "print(top5.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372b0858-1411-4369-a845-8bf238ebcefd",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03e19e3b-7c97-4964-887b-5ce3366d2edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model:\n",
    "\n",
    "import pickle\n",
    "pickle.dump(pipeline, open(\"trained_model.pkl\", \"wb\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776f822f-33f3-4c50-9afa-1b428fff24a9",
   "metadata": {},
   "source": [
    "## loading a saved model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3eb81c6c-ec23-4c0a-a039-cd430c279c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = pickle.load(open(\"trained_model.pkl\",\"rb\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
